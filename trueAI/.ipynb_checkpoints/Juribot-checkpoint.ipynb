{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/david/anaconda/lib/python2.7/site-packages/numexpr/cpuinfo.py:76: UserWarning: [Errno 2] No such file or directory\n",
      "  stacklevel=stacklevel + 1):\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords as wn\n",
    "from nltk.corpus import wordnet\n",
    "from string import maketrans\n",
    "\n",
    "import sklearn.datasets as datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Question_set = pd.read_csv('./questions.csv',sep=';')\n",
    "all_docs= [q for q in Question_set['Question']]\n",
    "all_labels = [l for l in Question_set['Categorie']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import download\n",
    "\n",
    "def nltkDownload():\n",
    "    \"\"\" Vous n'aurez surement pas les stopwords, il vous faudra les télécharger.\n",
    "    \"\"\"\n",
    "    download()\n",
    "\n",
    "def fromVectoBow(all_docs, vec, normalize=True):\n",
    "    bow = vec.fit_transform(all_docs)\n",
    "    bow = bow.tocsr() # permet de print\n",
    "    if normalize:\n",
    "        bow = normalizeBow(bow)\n",
    "    return bow\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def makeNltkStopWords(languages=['french', 'english', 'german', 'spanish']):\n",
    "    stop_words = []\n",
    "    for l in languages:\n",
    "        for w in stopwords.words(l):\n",
    "            stop_words.append(w.encode('utf-8')) #w.decode('utf-8') buggait... avec certains caractères\n",
    "    return stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import sklearn.feature_extraction.text as txt\n",
    "    \n",
    "def fromAllDocsToBow(all_docs, strip_accents=u'ascii', lowercase=True, \\\n",
    "                     preprocessor=None, stop_words=None, token_pattern=u\"[\\\\w']+\\\\w\\\\b\", \\\n",
    "                     analyzer=u'word', max_df=1.0, max_features=20000, vocabulary=None, \\\n",
    "                     binary=False, ngram_range=(1, 1), min_df=1, \\\n",
    "                     normalize=True):\n",
    "    \"\"\" Depuis un liste de documents, génère une matrice sparse contenant les occurences des mots.\n",
    "        A chaque mot est associé un identifiant grace à une table de hashage.\n",
    "    \"\"\"\n",
    "    vec_param = txt.CountVectorizer(all_docs, strip_accents=strip_accents, lowercase=lowercase, preprocessor=preprocessor, \\\n",
    "                            stop_words=stop_words, token_pattern=token_pattern, analyzer=analyzer, max_df=max_df, \\\n",
    "                            max_features=max_features, vocabulary=vocabulary, binary=binary, ngram_range=ngram_range, \\\n",
    "                            min_df=min_df)\n",
    "    bow = fromVectoBow(all_docs, vec_param, normalize)\n",
    "    return bow, vec_param\n",
    "\n",
    "def fromVectoBow(all_docs, vec, normalize=True):\n",
    "    bow = vec.fit_transform(all_docs)\n",
    "    bow = bow.tocsr() # permet de print\n",
    "    if normalize:\n",
    "        bow = normalizeBow(bow)\n",
    "    return bow\n",
    "\n",
    "def normalizeBow(bow):\n",
    "    \"\"\" TFIDF : La somme de toutes les occurences des mots devient égale à 1\n",
    "    \"\"\"\n",
    "    transformer = txt.TfidfTransformer(use_idf=False, smooth_idf=False)\n",
    "    bow_norm = transformer.fit_transform(bow)\n",
    "    return bow_norm   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.naive_bayes as nb\n",
    "from sklearn import svm\n",
    "from sklearn import linear_model as lin\n",
    "from sklearn import cross_validation\n",
    "from sklearn import tree\n",
    "\n",
    "\n",
    "def crossValidation(clf, bow, all_labels, cv=5):\n",
    "    X = bow\n",
    "    y = all_labels\n",
    "    scores = cross_validation.cross_val_score(clf, X, y, cv=5)\n",
    "    np_scores = np.array(scores)\n",
    "    mean = np_scores.mean()\n",
    "    std = np_scores.std()\n",
    "    return scores, mean, std \n",
    "\n",
    "def fit(clf, bow, all_labels):\n",
    "    \"\"\" Indispensable pour obtenir les clf.coef_ utile à la descrimination des mots \"\"\"\n",
    "    X = bow\n",
    "    y = all_labels\n",
    "    clf.fit(X, y)\n",
    "    return clf\n",
    "\n",
    "def predict(clf, docs):\n",
    "    return clf.predict(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mots vectorisés du second document :\n",
      "  (0, 7)\t0.57735026919\n",
      "  (0, 3)\t0.57735026919\n",
      "  (0, 5)\t0.57735026919\n",
      "  (1, 3)\t0.707106781187\n",
      "  (1, 5)\t0.707106781187\n",
      "  (2, 3)\t0.57735026919\n",
      "  (2, 5)\t0.57735026919\n",
      "  (2, 6)\t0.57735026919\n",
      "  (3, 7)\t0.707106781187\n",
      "  (3, 1)\t0.707106781187\n",
      "  (4, 1)\t1.0\n",
      "  (5, 7)\t0.5\n",
      "  (5, 3)\t0.5\n",
      "  (5, 5)\t0.5\n",
      "  (5, 6)\t0.5\n",
      "  (6, 7)\t0.5\n",
      "  (6, 3)\t0.5\n",
      "  (6, 5)\t0.5\n",
      "  (6, 6)\t0.5\n",
      "  (7, 6)\t0.707106781187\n",
      "  (7, 1)\t0.707106781187\n",
      "  (8, 3)\t0.707106781187\n",
      "  (8, 5)\t0.707106781187\n",
      "  (9, 7)\t0.707106781187\n",
      "  (9, 1)\t0.707106781187\n",
      "  :\t:\n",
      "  (12, 1)\t1.0\n",
      "  (13, 2)\t1.0\n",
      "  (14, 2)\t1.0\n",
      "  (15, 4)\t0.707106781187\n",
      "  (15, 0)\t0.707106781187\n",
      "  (16, 4)\t1.0\n",
      "  (18, 4)\t0.707106781187\n",
      "  (18, 0)\t0.707106781187\n",
      "  (19, 6)\t0.57735026919\n",
      "  (19, 4)\t0.57735026919\n",
      "  (19, 0)\t0.57735026919\n",
      "  (20, 2)\t1.0\n",
      "  (21, 0)\t1.0\n",
      "  (22, 0)\t1.0\n",
      "  (23, 6)\t0.707106781187\n",
      "  (23, 0)\t0.707106781187\n",
      "  (25, 2)\t0.707106781187\n",
      "  (25, 4)\t0.707106781187\n",
      "  (26, 4)\t1.0\n",
      "  (27, 4)\t0.707106781187\n",
      "  (27, 0)\t0.707106781187\n",
      "  (28, 2)\t0.707106781187\n",
      "  (28, 4)\t0.707106781187\n",
      "  (30, 2)\t1.0\n",
      "  (32, 2)\t1.0\n"
     ]
    }
   ],
   "source": [
    "# Paramétrage\n",
    "languages = ['french', 'english', 'german', 'spanish']\n",
    "stop_words = makeNltkStopWords(languages) # si erreur executez nltkDownload()\n",
    "analyzer = u'word' # {‘word’, ‘char’, ‘char_wb’}\n",
    "ngram_range = (1, 1) # unigrammes\n",
    "lowercase = True\n",
    "token_pattern = u\"[\\\\w']+\\\\w\\\\b\" # \n",
    "max_df = 1.0 #default\n",
    "min_df = 5. * 1./len(all_docs) # on enleve les mots qui apparaissent moins de 5 fois\n",
    "max_features = 20000 # nombre de mots au total dans notre matrice sparse\n",
    "binary = False # presence coding or counting\n",
    "strip_accents = u'ascii' #  {‘ascii’, ‘unicode’, None}\n",
    "preprocessor=None\n",
    "vocabulary=None\n",
    "\n",
    "# Vectorisation\n",
    "bow, vec = fromAllDocsToBow(all_docs, strip_accents=strip_accents, lowercase=lowercase, preprocessor=preprocessor, \\\n",
    "                            stop_words=stop_words, token_pattern=token_pattern, analyzer=analyzer, max_df=max_df, \\\n",
    "                            max_features=max_features, vocabulary=vocabulary, binary=binary, ngram_range=ngram_range, \\\n",
    "                            min_df=min_df)\n",
    "\n",
    "print \"Mots vectorisés du second document :\"\n",
    "print bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On crée des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Test avec la normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Après normalisation :\n",
      "  (0, 3)\t0.707106781187\n",
      "  (0, 5)\t0.707106781187\n"
     ]
    }
   ],
   "source": [
    "# Normalisation\n",
    "bow = normalizeBow(bow)\n",
    "\n",
    "print \"Après normalisation :\"\n",
    "print bow[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On implémente nos modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores obtenus avec crossValidation : [ 0.88888889  0.75        0.75        0.71428571  1.        ]\n",
      "Moyenne : 0.820634920635\n",
      "Ecart type : 0.107813889268\n"
     ]
    }
   ],
   "source": [
    "dt = tree.DecisionTreeClassifier()\n",
    "# Modèles\n",
    "clf = svm.LinearSVC() # SVM\n",
    "clf_nb = nb.MultinomialNB() # Naive Bayes\n",
    "clf_rl = lin.LogisticRegression() # regression logistique\n",
    "\n",
    "# Cross-Validation\n",
    "scores, mean, std  = crossValidation(dt, bow, all_labels, cv=5)\n",
    "\n",
    "print \"Scores obtenus avec crossValidation :\", scores\n",
    "print \"Moyenne :\", mean\n",
    "print \"Ecart type :\", std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test d'un nouveau bot avec de nouvelles données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'getAllDocs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-487b32950c48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mall_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetAllDocs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m comment = \"\"\"Automata' (2014) is a critically underrated and atmospheric science- fiction thriller in the same vein as 'I Robot' and 'Blade Runner'. It boasts excellent visual effects, as well as an engaging and intelligent story. While it borrows from other science fiction it does so successfully, especially the atmospheric and decaying world we're thrusts into from the beginning.\n\u001b[1;32m      4\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mstory\u001b[0m \u001b[0mcenters\u001b[0m \u001b[0maround\u001b[0m \u001b[0mAntonio\u001b[0m \u001b[0mBanderas\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mcharacter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mJacq\u001b[0m \u001b[0mVaucan\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0ma\u001b[0m \u001b[0mworld\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mweary\u001b[0m \u001b[0minsurance\u001b[0m \u001b[0magent\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0mrobotics\u001b[0m \u001b[0mcorporation\u001b[0m \u001b[0mwhose\u001b[0m \u001b[0mjob\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mto\u001b[0m \u001b[0minvestigate\u001b[0m \u001b[0mrobots\u001b[0m \u001b[0mviolating\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mprotocols\u001b[0m \u001b[0mwhich\u001b[0m \u001b[0mare\u001b[0m \u001b[0mone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mharming\u001b[0m \u001b[0many\u001b[0m \u001b[0mform\u001b[0m \u001b[0mof\u001b[0m \u001b[0mlife\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtwo\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mcan\u001b[0m \u001b[0mneither\u001b[0m \u001b[0mrepair\u001b[0m \u001b[0mthemselves\u001b[0m \u001b[0mnor\u001b[0m \u001b[0malter\u001b[0m \u001b[0manother\u001b[0m \u001b[0mrobot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0many\u001b[0m \u001b[0mfashion\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mOn\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtrail\u001b[0m \u001b[0mof\u001b[0m \u001b[0ma\u001b[0m \u001b[0mrobot\u001b[0m \u001b[0mVaucan\u001b[0m \u001b[0mdiscovers\u001b[0m \u001b[0ma\u001b[0m \u001b[0mrobot\u001b[0m \u001b[0mstealing\u001b[0m \u001b[0mparts\u001b[0m \u001b[0;32min\u001b[0m \u001b[0man\u001b[0m \u001b[0mapparent\u001b[0m \u001b[0mattempt\u001b[0m \u001b[0mto\u001b[0m \u001b[0malter\u001b[0m \u001b[0mitself\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mThis\u001b[0m \u001b[0mleads\u001b[0m \u001b[0mhim\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mclock\u001b[0m \u001b[0mmaster\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0ma\u001b[0m \u001b[0mfixer\u001b[0m \u001b[0mwho\u001b[0m \u001b[0mmay\u001b[0m \u001b[0mhave\u001b[0m \u001b[0mjust\u001b[0m \u001b[0msucceeded\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msecond\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mAutomata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mthrowback\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthoughtful\u001b[0m \u001b[0mscience\u001b[0m \u001b[0mfiction\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mIt\u001b[0m\u001b[0;34m's not for the feint of heart but if you'\u001b[0m\u001b[0mre\u001b[0m \u001b[0mengaged\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbuy\u001b[0m \u001b[0minto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mworld\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpremise\u001b[0m \u001b[0mthen\u001b[0m \u001b[0myou\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0mll\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mrewarded\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mfilm\u001b[0m \u001b[0msurprised\u001b[0m \u001b[0mme\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlot\u001b[0m \u001b[0mof\u001b[0m \u001b[0mways\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'getAllDocs' is not defined"
     ]
    }
   ],
   "source": [
    "all_docs = getAllDocs(docs)\n",
    "\n",
    "comment = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "comment_bad = \"\"\"attempts\"\"\"\n",
    "\n",
    "comment_good = \"\"\"enjoyable\"\"\"\n",
    "\n",
    "all_docs.append(comment)\n",
    "all_docs.append(comment_bad)\n",
    "all_docs.append(comment_good)\n",
    "len_docs = len(all_docs)\n",
    "\n",
    "bow, vec = fromAllDocsToBow(all_docs, strip_accents=strip_accents, lowercase=lowercase, preprocessor=preprocessor, \\\n",
    "                            stop_words=stop_words, token_pattern=token_pattern, analyzer=analyzer, max_df=max_df, \\\n",
    "                            max_features=max_features, vocabulary=vocabulary, binary=binary, ngram_range=ngram_range, \\\n",
    "                            min_df=min_df)\n",
    "\n",
    "clf = fit(clf, bow[:len_docs-3], all_labels)\n",
    "\n",
    "pred = predict(clf, bow[len_docs-3])\n",
    "print \"Classe du commentaire Automata (bon) :\", pred\n",
    "\n",
    "pred = predict(clf, bow[len_docs-2])\n",
    "print \"Classe du commentaire mauvais :\", pred\n",
    "\n",
    "pred = predict(clf, bow[len_docs-1])\n",
    "print \"Classe du commentaire bon :\", pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
