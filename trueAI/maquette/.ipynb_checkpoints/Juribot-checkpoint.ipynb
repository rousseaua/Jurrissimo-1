{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords as wn\n",
    "from nltk.corpus import wordnet\n",
    "from string import maketrans\n",
    "\n",
    "import sklearn.datasets as datasets\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Question_set = pd.read_csv('./questions.csv',sep=';')\n",
    "all_docs= [q for q in Question_set['Question']]\n",
    "all_labels = [l for l in Question_set['Categorie']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import download\n",
    "\n",
    "def nltkDownload():\n",
    "    \"\"\" Vous n'aurez surement pas les stopwords, il vous faudra les télécharger.\n",
    "    \"\"\"\n",
    "    download()\n",
    "\n",
    "def fromVectoBow(all_docs, vec, normalize=True):\n",
    "    bow = vec.fit_transform(all_docs)\n",
    "    bow = bow.tocsr() # permet de print\n",
    "    if normalize:\n",
    "        bow = normalizeBow(bow)\n",
    "    return bow\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def makeNltkStopWords(languages=['french', 'english', 'german', 'spanish']):\n",
    "    stop_words = []\n",
    "    for l in languages:\n",
    "        for w in stopwords.words(l):\n",
    "            stop_words.append(w.encode('utf-8')) #w.decode('utf-8') buggait... avec certains caractères\n",
    "    return stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import sklearn.feature_extraction.text as txt\n",
    "    \n",
    "def fromAllDocsToBow(all_docs, strip_accents=u'ascii', lowercase=True, \\\n",
    "                     preprocessor=None, stop_words=None, token_pattern=u\"[\\\\w']+\\\\w\\\\b\", \\\n",
    "                     analyzer=u'word', max_df=1.0, max_features=20000, vocabulary=None, \\\n",
    "                     binary=False, ngram_range=(1, 1), min_df=1, \\\n",
    "                     normalize=True):\n",
    "    \"\"\" Depuis un liste de documents, génère une matrice sparse contenant les occurences des mots.\n",
    "        A chaque mot est associé un identifiant grace à une table de hashage.\n",
    "    \"\"\"\n",
    "    vec_param = txt.CountVectorizer(all_docs, strip_accents=strip_accents, lowercase=lowercase, preprocessor=preprocessor, \\\n",
    "                            stop_words=stop_words, token_pattern=token_pattern, analyzer=analyzer, max_df=max_df, \\\n",
    "                            max_features=max_features, vocabulary=vocabulary, binary=binary, ngram_range=ngram_range, \\\n",
    "                            min_df=min_df)\n",
    "    bow = fromVectoBow(all_docs, vec_param, normalize)\n",
    "    return bow, vec_param\n",
    "\n",
    "def fromVectoBow(all_docs, vec, normalize=True):\n",
    "    bow = vec.fit_transform(all_docs)\n",
    "    bow = bow.tocsr() # permet de print\n",
    "    if normalize:\n",
    "        bow = normalizeBow(bow)\n",
    "    return bow\n",
    "\n",
    "def normalizeBow(bow):\n",
    "    \"\"\" TFIDF : La somme de toutes les occurences des mots devient égale à 1\n",
    "    \"\"\"\n",
    "    transformer = txt.TfidfTransformer(use_idf=False, smooth_idf=False)\n",
    "    bow_norm = transformer.fit_transform(bow)\n",
    "    return bow_norm   \n",
    "\n",
    "def message(pred):\n",
    "    if prediction == 0 :\n",
    "        print \"Il s'agit d'une question sur les motifs\\n\"\n",
    "        sleep(3)\n",
    "        print \"Le propriétaire peut résilier un bail locatif :\\n\"\n",
    "        sleep(3)\n",
    "        print \"s'il veut reprendre le logement pour y habiter ou loger un proche, s'il veut le vendre, ou pour les motifs sérieux suivants :\\n\"\n",
    "        sleep(3)\n",
    "        print \"(non-paiement du loyer, non-souscription à une assurance de risques locatifs, troubles du voisinage).\\n\"\n",
    "        sleep(3)\n",
    "        print \"S'il résilie le contrat pour une autre raison.\\n\"\n",
    "        print \"Je vous conseille les voies de recours suivantes : https://www.service-public.fr/particuliers/vosdroits/F31301.\"\n",
    "    elif prediction  == 1:\n",
    "        print \"Il s'agit d'une question sur les délais\\n\"\n",
    "        sleep(2)\n",
    "        print \"Pour les contrats de bail d'une location nue ou meublée, la loi prévoit que le dépôt de garantie doit être restitué sous deux mois à compter de la restitution des clefs.\"\n",
    "        sleep(2)\n",
    "        print \"Sous un mois si l'état des lieux de sortie correspond à celui établi à l'entrée\"\n",
    "        sleep(2)\n",
    "        print \"En cas de difficultés dans la restitution du dépôt de garantie sous le délai indiqué, je vous conseille de lire l'article suivant https://www.service-public.fr/particuliers/vosdroits/F31301.\"\n",
    "    elif prediction == 3:\n",
    "        print \"Haha, you're a funny guy.\"\n",
    "        sleep(2)\n",
    "        print \"I'm very sorry, but I don't know anything about the american law\"\n",
    "\n",
    "    elif prediction == 2:\n",
    "        print \"Il s'agit d'une question sur les contrats de travail.\"\n",
    "        sleep(2)\n",
    "        print \"Je suis désolé, je ne connais pas réellement de sujet.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.naive_bayes as nb\n",
    "from sklearn import svm\n",
    "from sklearn import linear_model as lin\n",
    "from sklearn import cross_validation\n",
    "from sklearn import tree\n",
    "\n",
    "all_docs= [q for q in Question_set['Question']]\n",
    "all_labels = [l for l in Question_set['Categorie']]\n",
    "\n",
    "\n",
    "def crossValidation(clf, bow, all_labels, cv=5):\n",
    "    X = bow\n",
    "    y = all_labels\n",
    "    scores = cross_validation.cross_val_score(clf, X, y, cv=5)\n",
    "    np_scores = np.array(scores)\n",
    "    mean = np_scores.mean()\n",
    "    std = np_scores.std()\n",
    "    return scores, mean, std \n",
    "\n",
    "def fit(clf, bow, all_labels):\n",
    "    \"\"\" Indispensable pour obtenir les clf.coef_ utile à la descrimination des mots \"\"\"\n",
    "    X = bow\n",
    "    y = all_labels\n",
    "    clf.fit(X, y)\n",
    "    return clf\n",
    "\n",
    "def predict(clf, docs):\n",
    "    return clf.predict(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Paramétrage\n",
    "languages = ['french']\n",
    "stop_words = makeNltkStopWords(languages) # si erreur executez nltkDownload()\n",
    "analyzer = u'word' # {‘word’, ‘char’, ‘char_wb’}\n",
    "ngram_range = (1, 1) # unigrammes\n",
    "lowercase = True\n",
    "token_pattern = u\"[\\\\w']+\\\\w\\\\b\" # \n",
    "max_df = 1.0 #default\n",
    "min_df = 2. * 1./len(all_docs) # on enleve les mots qui apparaissent moins de 5 fois\n",
    "max_features = 20000 # nombre de mots au total dans notre matrice sparse\n",
    "binary = False # presence coding or counting\n",
    "strip_accents = u'ascii' #  {‘ascii’, ‘unicode’, None}\n",
    "preprocessor=None\n",
    "vocabulary=None\n",
    "\n",
    "# Vectorisation\n",
    "bow, vec = fromAllDocsToBow(all_docs, strip_accents=strip_accents, lowercase=lowercase, preprocessor=preprocessor, \\\n",
    "                            stop_words=stop_words, token_pattern=token_pattern, analyzer=analyzer, max_df=max_df, \\\n",
    "                            max_features=max_features, vocabulary=vocabulary, binary=binary, ngram_range=ngram_range, \\\n",
    "                            min_df=min_df)\n",
    "\n",
    "#print \"Mots vectorisés du second document :\"\n",
    "#print bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On crée des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Test avec la normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Normalisation\n",
    "bow = normalizeBow(bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On implémente nos modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM\n",
      "Scores obtenus avec crossValidation : [ 0.92307692  1.          0.83333333  0.7         0.88888889]\n",
      "Moyenne : 0.86905982906\n",
      "Ecart type : 0.100313483365\n",
      "naive bayes\n",
      "Scores obtenus avec crossValidation : [ 1.          0.92307692  0.83333333  0.9         0.88888889]\n",
      "Moyenne : 0.90905982906\n",
      "Ecart type : 0.0542034004474\n",
      "logistic regression\n",
      "Scores obtenus avec crossValidation : [ 1.          0.92307692  0.83333333  0.9         0.88888889]\n",
      "Moyenne : 0.90905982906\n",
      "Ecart type : 0.0542034004474\n",
      "decision tree\n",
      "Scores obtenus avec crossValidation : [ 0.84615385  1.          0.91666667  0.6         0.55555556]\n",
      "Moyenne : 0.783675213675\n",
      "Ecart type : 0.175591504238\n"
     ]
    }
   ],
   "source": [
    "Question_set = pd.read_csv('./questions.csv',sep=';')\n",
    "all_docs= [q for q in Question_set['Question']]\n",
    "all_labels = [l for l in Question_set['Categorie']]\n",
    "\n",
    "dt = tree.DecisionTreeClassifier()\n",
    "\n",
    "# Modèles\n",
    "clf = svm.LinearSVC() # SVM\n",
    "clf_nb = nb.MultinomialNB() # Naive Bayes\n",
    "clf_rl = lin.LogisticRegression() # regression logistique\n",
    "\n",
    "\n",
    "bow, vec = fromAllDocsToBow(all_docs, strip_accents=strip_accents, lowercase=lowercase, preprocessor=preprocessor, \\\n",
    "                            stop_words=stop_words, token_pattern=token_pattern, analyzer=analyzer, max_df=max_df, \\\n",
    "                            max_features=max_features, vocabulary=vocabulary, binary=binary, ngram_range=ngram_range, \\\n",
    "                            min_df=min_df)\n",
    "\n",
    "models =[{'name': 'SVM','model' :clf},{'name' : 'naive bayes' , 'model' : clf_nb},{'name' : 'logistic regression','model' : clf_rl},{'name':'decision tree','model':dt}]\n",
    "# Cross-Validation\n",
    "for model in models:\n",
    "    print model['name']\n",
    "    scores, mean, std  = crossValidation(model['model'], bow, all_labels, cv=2)\n",
    "    print \"Scores obtenus avec crossValidation :\", scores\n",
    "    print \"Moyenne :\", mean\n",
    "    print \"Ecart type :\", std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test d'un nouveau bot avec de nouvelles données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il s'agit d'une question sur les délais\n",
      "\n",
      "Pour les contrats de bail d'une location nue ou meublée, la loi prévoit que le dépôt de garantie doit être restitué sous deux mois à compter de la restitution des clefs.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-323-ed76f4e53dfc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mmessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-298-6b4006737c74>\u001b[0m in \u001b[0;36mmessage\u001b[0;34m(pred)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Pour les contrats de bail d'une location nue ou meublée, la loi prévoit que le dépôt de garantie doit être restitué sous deux mois à compter de la restitution des clefs.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Sous un mois si l'état des lieux de sortie correspond à celui établi à l'entrée\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Motifs; délais; contrat-de-travail ; \n",
    "Question_set = pd.read_csv('./questions.csv',sep=';')\n",
    "all_docs= [q for q in Question_set['Question']]\n",
    "all_labels = [l for l in Question_set['Categorie']]\n",
    "\n",
    "comment = \"les\"\n",
    "\n",
    "\n",
    "all_docs.append(comment)\n",
    "\n",
    "len_docs = len(all_docs)\n",
    "\n",
    "bow, vec = fromAllDocsToBow(all_docs, strip_accents=strip_accents, lowercase=lowercase, preprocessor=preprocessor, \\\n",
    "                            stop_words=stop_words, token_pattern=token_pattern, analyzer=analyzer, max_df=max_df, \\\n",
    "                            max_features=max_features, vocabulary=vocabulary, binary=binary, ngram_range=ngram_range, \\\n",
    "                            min_df=min_df)\n",
    "\n",
    "clf = fit(clf, bow[:len_docs-1], all_labels)\n",
    "\n",
    "#print 'Longueur des documents '+ len_docs\n",
    "\n",
    "pred = predict(clf, bow[len_docs-1])\n",
    "#print \"Classe du commentaire : \", pred[0]\n",
    "\n",
    "\n",
    "prediction = pred[0]\n",
    "\n",
    "message(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get new document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#all_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method score in module sklearn.base:\n",
      "\n",
      "score(self, X, y, sample_weight=None) method of sklearn.svm.classes.LinearSVC instance\n",
      "    Returns the mean accuracy on the given test data and labels.\n",
      "    \n",
      "    In multi-label classification, this is the subset accuracy\n",
      "    which is a harsh metric since you require for each sample that\n",
      "    each label set be correctly predicted.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    X : array-like, shape = (n_samples, n_features)\n",
      "        Test samples.\n",
      "    \n",
      "    y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "        True labels for X.\n",
      "    \n",
      "    sample_weight : array-like, shape = [n_samples], optional\n",
      "        Sample weights.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    score : float\n",
      "        Mean accuracy of self.predict(X) wrt. y.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(clf.score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
